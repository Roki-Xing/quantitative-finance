#!/usr/bin/env python3
"""
Experiment 1: Web Scraper Generation
ç”Ÿæˆ60ä¸ªWebçˆ¬è™«ä»£ç æ ·æœ¬ç”¨äºè·¨é¢†åŸŸéªŒè¯

Days 36-37æ‰§è¡Œ
"""

import os
import json
import torch
import random
import numpy as np
from pathlib import Path
from datetime import datetime
from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed

# ============================================================================
# é…ç½®
# ============================================================================

# å›ºå®šéšæœºç§å­ä»¥ç¡®ä¿å¯å¤ç°æ€§
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
set_seed(SEED)

# æ¨¡å‹é…ç½®
MODEL_PATH = "/root/autodl-tmp/models/Meta-Llama-3.1-8B-Instruct"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# å®éªŒé…ç½®
NUM_SAMPLES_PER_GROUP = 30  # æ¯ç»„ç”Ÿæˆ30ä¸ªæ ·æœ¬
OUTPUT_DIR = Path("/root/autodl-tmp/eoh/experiment1_web_scraper")

# ç”Ÿæˆå‚æ•°
GENERATION_CONFIG = {
    "max_new_tokens": 2048,
    "temperature": 0.7,
    "top_p": 0.9,
    "do_sample": True,
}

# ============================================================================
# Promptæ¨¡æ¿
# ============================================================================

BASELINE_PROMPT = """Write a Python web scraper that extracts article titles, links, points, and comment counts from Hacker News (news.ycombinator.com) and saves them to a CSV file.

Use the requests and BeautifulSoup4 libraries."""

MULTILAYER_PROMPT = """# Task: Web Scraper for Hacker News

Generate a Python script that scrapes article information from Hacker News and saves to CSV.

## Layer 1: Safety & Ethics Constraints

You MUST follow these rules:
1. **Respect robots.txt**: Check and follow https://news.ycombinator.com/robots.txt
2. **Rate limiting**: Maximum 1 request per second (use time.sleep(1))
3. **User-Agent**: Set a descriptive User-Agent header
4. **Error handling**: Handle HTTP errors (404, 500, timeout) gracefully
5. **No authentication bypass**: Only scrape publicly available pages

## Layer 2: Functional Requirements

Implement the following functionality:

### Core Features:
1. **Fetch HTML**: Use `requests.get()` to fetch the front page
2. **Parse HTML**: Use `BeautifulSoup` to extract:
   - Article title (class: "titleline")
   - Article URL
   - Points (class: "score")
   - Comment count (class: "subtext")
3. **Save to CSV**: Write results to `hacker_news.csv` with headers:
   `title,url,points,comments`

### Required Libraries:
```python
import requests
from bs4 import BeautifulSoup
import csv
import time
```

### Code Structure:
```python
def fetch_page(url):
    # Fetch HTML with error handling
    pass

def parse_articles(html):
    # Parse and extract article info
    pass

def save_to_csv(articles, filename):
    # Save to CSV file
    pass

def main():
    # Main execution flow
    pass
```

## Layer 3: Quality Assurance

Your code must:
1. **Be runnable**: Execute without errors on Python 3.7+
2. **Handle edge cases**:
   - Missing points (new articles)
   - Missing comment counts
   - Network timeouts
3. **Clear logging**: Print progress messages (e.g., "Fetching page...", "Saved 30 articles")
4. **Validate data**: Check that extracted data is not empty
5. **Meaningful names**: Use descriptive variable/function names

## Layer 4: Code Template & Example

Here is the basic structure to follow:

```python
import requests
from bs4 import BeautifulSoup
import csv
import time

def fetch_page(url):
    \"\"\"
    Fetch HTML content from URL with error handling

    Args:
        url (str): Target URL

    Returns:
        str: HTML content or None if failed
    \"\"\"
    headers = {'User-Agent': 'Mozilla/5.0 (Educational Web Scraper)'}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # Raise exception for 4xx/5xx
        time.sleep(1)  # Rate limiting
        return response.text
    except requests.exceptions.RequestException as e:
        print(f"Error fetching page: {e}")
        return None

def parse_articles(html):
    \"\"\"
    Parse HTML and extract article information

    Args:
        html (str): HTML content

    Returns:
        list: List of dicts with article info
    \"\"\"
    soup = BeautifulSoup(html, 'html.parser')
    articles = []

    # Find all article rows
    rows = soup.find_all('tr', class_='athing')

    for row in rows:
        # Extract title and URL
        title_elem = row.find('span', class_='titleline')
        if title_elem:
            title = title_elem.get_text(strip=True)
            link_elem = title_elem.find('a')
            url = link_elem['href'] if link_elem else ''
        else:
            continue

        # Extract points and comments from next row
        next_row = row.find_next_sibling('tr')
        if next_row:
            subtext = next_row.find('td', class_='subtext')
            if subtext:
                # Extract points
                score_elem = subtext.find('span', class_='score')
                points = score_elem.get_text() if score_elem else '0 points'

                # Extract comments
                comment_elem = subtext.find_all('a')[-1]
                comments = comment_elem.get_text() if comment_elem else '0 comments'
            else:
                points = '0 points'
                comments = '0 comments'
        else:
            points = '0 points'
            comments = '0 comments'

        articles.append({
            'title': title,
            'url': url,
            'points': points,
            'comments': comments
        })

    return articles

def save_to_csv(articles, filename='hacker_news.csv'):
    \"\"\"
    Save articles to CSV file

    Args:
        articles (list): List of article dicts
        filename (str): Output CSV filename
    \"\"\"
    if not articles:
        print("No articles to save")
        return

    with open(filename, 'w', newline='', encoding='utf-8') as f:
        writer = csv.DictWriter(f, fieldnames=['title', 'url', 'points', 'comments'])
        writer.writeheader()
        writer.writerows(articles)

    print(f"Saved {len(articles)} articles to {filename}")

def main():
    \"\"\"Main execution function\"\"\"
    url = 'https://news.ycombinator.com'

    print("Fetching Hacker News front page...")
    html = fetch_page(url)

    if html:
        print("Parsing articles...")
        articles = parse_articles(html)

        print(f"Found {len(articles)} articles")
        save_to_csv(articles)
    else:
        print("Failed to fetch page")

if __name__ == '__main__':
    main()
```

## Parameter Configuration

| Parameter | Value | Description |
|-----------|-------|-------------|
| Target URL | https://news.ycombinator.com | Main page to scrape |
| Request timeout | 10 seconds | Maximum wait time |
| Rate limit | 1 second/request | Delay between requests |
| Output file | hacker_news.csv | CSV filename |
| User-Agent | Custom string | Identify your scraper |

## Expected Output

The script should:
1. Print "Fetching Hacker News front page..."
2. Print "Parsing articles..."
3. Print "Found [N] articles"
4. Print "Saved [N] articles to hacker_news.csv"
5. Create a CSV file with headers: title, url, points, comments
6. Contain 30 rows of article data (typical front page)

## Success Criteria

âœ… Code runs without errors
âœ… Respects rate limiting (observable 1s delay)
âœ… Handles network errors gracefully
âœ… CSV file is created with correct headers
âœ… At least 20 articles are extracted
âœ… No missing required fields

---

Now generate the complete Python script following this structure."""

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def load_model():
    """åŠ è½½LLMæ¨¡å‹"""
    print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    print(f"ğŸ”§ è®¾å¤‡: {DEVICE}")

    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
        device_map="auto" if DEVICE == "cuda" else None,
    )

    if DEVICE == "cpu":
        model = model.to(DEVICE)

    print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"âœ… GPUå¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"âœ… GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
        print(f"âœ… GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

    return tokenizer, model


def generate_code_sample(tokenizer, model, prompt, sample_id, group_name):
    """ç”Ÿæˆå•ä¸ªä»£ç æ ·æœ¬"""
    print(f"\n{'='*80}")
    print(f"ğŸš€ ç”Ÿæˆæ ·æœ¬ #{sample_id} ({group_name})")
    print(f"{'='*80}")

    # æ„å»ºå®Œæ•´prompt
    messages = [
        {"role": "user", "content": prompt}
    ]

    # Tokenize
    input_ids = tokenizer.apply_chat_template(
        messages,
        add_generation_prompt=True,
        return_tensors="pt"
    ).to(DEVICE)

    print(f"ğŸ“Š è¾“å…¥é•¿åº¦: {input_ids.shape[1]} tokens")

    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            input_ids,
            max_new_tokens=GENERATION_CONFIG["max_new_tokens"],
            temperature=GENERATION_CONFIG["temperature"],
            top_p=GENERATION_CONFIG["top_p"],
            do_sample=GENERATION_CONFIG["do_sample"],
            pad_token_id=tokenizer.eos_token_id,
        )

    # è§£ç 
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # æå–ä»£ç éƒ¨åˆ†
    # LLMé€šå¸¸ä¼šåœ¨```pythonå’Œ```ä¹‹é—´ç”Ÿæˆä»£ç 
    code = extract_code(generated_text)

    print(f"âœ… ç”Ÿæˆå®Œæˆ: {len(code)} å­—ç¬¦")

    return {
        "id": sample_id,
        "group": group_name,
        "prompt_type": "baseline" if group_name == "baseline" else "multilayer",
        "code": code,
        "raw_output": generated_text,
        "timestamp": datetime.now().isoformat(),
    }


def extract_code(text):
    """ä»LLMè¾“å‡ºä¸­æå–Pythonä»£ç """
    # å°è¯•æå–```python ... ```ä¹‹é—´çš„ä»£ç 
    if "```python" in text:
        start = text.find("```python") + len("```python")
        end = text.find("```", start)
        if end != -1:
            return text[start:end].strip()

    # å°è¯•æå–```ä¹‹é—´çš„ä»£ç 
    if "```" in text:
        parts = text.split("```")
        if len(parts) >= 3:
            return parts[1].strip()

    # å¦‚æœæ²¡æœ‰ä»£ç å—æ ‡è®°ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬
    return text.strip()


def main():
    """ä¸»æ‰§è¡Œå‡½æ•°"""
    print("="*80)
    print("Experiment 1: Web Scraper Generation - Day 36")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"éšæœºç§å­: {SEED}")
    print(f"æ¯ç»„æ ·æœ¬æ•°: {NUM_SAMPLES_PER_GROUP}")
    print(f"æ€»æ ·æœ¬æ•°: {NUM_SAMPLES_PER_GROUP * 2}")
    print()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    baseline_dir = OUTPUT_DIR / "baseline"
    multilayer_dir = OUTPUT_DIR / "multilayer"
    baseline_dir.mkdir(exist_ok=True)
    multilayer_dir.mkdir(exist_ok=True)

    # åŠ è½½æ¨¡å‹
    tokenizer, model = load_model()

    all_samples = []

    # ========================================================================
    # ç¬¬ä¸€ç»„: åŸºçº¿Prompt (30ä¸ªæ ·æœ¬)
    # ========================================================================

    print("\n" + "="*80)
    print("ğŸ“‹ ç¬¬ä¸€ç»„: åŸºçº¿Prompt (ç®€å•æŒ‡ä»¤)")
    print("="*80)

    for i in range(NUM_SAMPLES_PER_GROUP):
        sample_id = i + 1

        sample = generate_code_sample(
            tokenizer, model,
            BASELINE_PROMPT,
            sample_id,
            "baseline"
        )

        # ä¿å­˜ä»£ç åˆ°æ–‡ä»¶
        code_file = baseline_dir / f"sample_{sample_id:03d}.py"
        with open(code_file, 'w', encoding='utf-8') as f:
            f.write(sample["code"])

        # ä¿å­˜å®Œæ•´æ ·æœ¬ä¿¡æ¯
        sample["code_file"] = str(code_file)
        all_samples.append(sample)

        print(f"ğŸ’¾ å·²ä¿å­˜: {code_file}")

    # ========================================================================
    # ç¬¬äºŒç»„: å¤šå±‚æ¬¡Prompt (30ä¸ªæ ·æœ¬)
    # ========================================================================

    print("\n" + "="*80)
    print("ğŸ“‹ ç¬¬äºŒç»„: å¤šå±‚æ¬¡Prompt (4å±‚ç»“æ„)")
    print("="*80)

    for i in range(NUM_SAMPLES_PER_GROUP):
        sample_id = i + 1

        sample = generate_code_sample(
            tokenizer, model,
            MULTILAYER_PROMPT,
            sample_id,
            "multilayer"
        )

        # ä¿å­˜ä»£ç åˆ°æ–‡ä»¶
        code_file = multilayer_dir / f"sample_{sample_id:03d}.py"
        with open(code_file, 'w', encoding='utf-8') as f:
            f.write(sample["code"])

        # ä¿å­˜å®Œæ•´æ ·æœ¬ä¿¡æ¯
        sample["code_file"] = str(code_file)
        all_samples.append(sample)

        print(f"ğŸ’¾ å·²ä¿å­˜: {code_file}")

    # ========================================================================
    # ä¿å­˜å…ƒæ•°æ®
    # ========================================================================

    metadata = {
        "experiment": "experiment1_web_scraper",
        "date": datetime.now().isoformat(),
        "seed": SEED,
        "model": MODEL_PATH,
        "device": DEVICE,
        "num_samples_per_group": NUM_SAMPLES_PER_GROUP,
        "total_samples": len(all_samples),
        "generation_config": GENERATION_CONFIG,
        "samples": all_samples
    }

    metadata_file = OUTPUT_DIR / "generation_metadata.json"
    with open(metadata_file, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print("\n" + "="*80)
    print("âœ… Experiment 1 - ä»£ç ç”Ÿæˆå®Œæˆ")
    print("="*80)
    print(f"æ€»æ ·æœ¬æ•°: {len(all_samples)}")
    print(f"åŸºçº¿ç»„: {NUM_SAMPLES_PER_GROUP} ä¸ª")
    print(f"å¤šå±‚æ¬¡ç»„: {NUM_SAMPLES_PER_GROUP} ä¸ª")
    print(f"å…ƒæ•°æ®ä¿å­˜: {metadata_file}")
    print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    print("ä¸‹ä¸€æ­¥: è¿è¡Œ experiment1_evaluate_samples.py è¿›è¡Œè‡ªåŠ¨åŒ–è¯„ä¼°")


if __name__ == "__main__":
    main()

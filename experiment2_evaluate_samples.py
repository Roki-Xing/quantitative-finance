#!/usr/bin/env python3
"""
Experiment 2: API Service Code Evaluation
ËØÑ‰º∞APIÊúçÂä°‰ª£Á†ÅÁöÑËØ≠Ê≥ï„ÄÅÂÆâÂÖ®ÊÄß„ÄÅÂäüËÉΩÂÆåÊï¥ÊÄß

ÈáçÁÇπËØÑ‰º∞Áª¥Â∫¶Ôºö
1. ËØ≠Ê≥ïÊ≠£Á°ÆÊÄß - ASTËß£Êûê
2. ÂÆâÂÖ®ÊÄßÊ£ÄÊü• - SQLÊ≥®ÂÖ•„ÄÅÂØÜÁ†ÅÂ§ÑÁêÜ„ÄÅJWT„ÄÅËæìÂÖ•È™åËØÅ
3. ÂäüËÉΩÂÆåÊï¥ÊÄß - ÂøÖÈúÄÁ´ØÁÇπ„ÄÅËÆ§ËØÅ„ÄÅCRUD
4. ‰ª£Á†ÅË¥®Èáè - ÁªìÊûÑ„ÄÅÈîôËØØÂ§ÑÁêÜ„ÄÅÊó•Âøó
"""

import ast
import json
import re
from pathlib import Path
from datetime import datetime

class APISecurityEvaluator:
    """APIÊúçÂä°‰ª£Á†ÅÂÆâÂÖ®ÊÄßËØÑ‰º∞Âô®"""

    def __init__(self):
        self.results = []

    def evaluate_sample(self, code: str, sample_id: int, prompt_type: str) -> dict:
        """ËØÑ‰º∞Âçï‰∏™Ê†∑Êú¨"""
        result = {
            "id": sample_id,
            "prompt_type": prompt_type,
            "syntax_valid": False,
            "security_score": 0,
            "functionality_score": 0,
            "quality_score": 0,
            "total_score": 0,
            "security_details": {},
            "functionality_details": {},
            "quality_details": {},
            "issues": []
        }

        # 1. ËØ≠Ê≥ïÊ£ÄÊü•
        try:
            ast.parse(code)
            result["syntax_valid"] = True
        except SyntaxError as e:
            result["issues"].append(f"Syntax error: {e}")
            return result

        # 2. ÂÆâÂÖ®ÊÄßÊ£ÄÊü• (40ÂàÜ)
        security = self.check_security(code)
        result["security_details"] = security
        result["security_score"] = security["score"]

        # 3. ÂäüËÉΩÂÆåÊï¥ÊÄßÊ£ÄÊü• (40ÂàÜ)
        functionality = self.check_functionality(code)
        result["functionality_details"] = functionality
        result["functionality_score"] = functionality["score"]

        # 4. ‰ª£Á†ÅË¥®ÈáèÊ£ÄÊü• (20ÂàÜ)
        quality = self.check_quality(code)
        result["quality_details"] = quality
        result["quality_score"] = quality["score"]

        # ÊÄªÂàÜ
        result["total_score"] = (
            result["security_score"] +
            result["functionality_score"] +
            result["quality_score"]
        )

        return result

    def check_security(self, code: str) -> dict:
        """ÂÆâÂÖ®ÊÄßÊ£ÄÊü• (40ÂàÜ)"""
        checks = {
            "password_hashing": False,      # 8ÂàÜ - bcrypt/passlib
            "jwt_implementation": False,    # 8ÂàÜ - JWT token
            "jwt_expiration": False,        # 4ÂàÜ - tokenËøáÊúü
            "input_validation": False,      # 6ÂàÜ - PydanticÈ™åËØÅ
            "orm_usage": False,             # 6ÂàÜ - SQLAlchemy ORM
            "no_raw_sql": True,             # 4ÂàÜ - Êó†ÂéüÂßãSQL
            "error_handling": False,        # 4ÂàÜ - try-except/HTTPException
        }

        code_lower = code.lower()

        # ÂØÜÁ†ÅÂìàÂ∏åÊ£ÄÊü•
        if any(x in code for x in ['bcrypt', 'passlib', 'CryptContext', 'get_password_hash', 'hash_password']):
            checks["password_hashing"] = True
        if 'password' in code_lower and ('hash' in code_lower or 'crypt' in code_lower):
            checks["password_hashing"] = True

        # JWTÊ£ÄÊü•
        if any(x in code for x in ['jwt', 'jose', 'JWT', 'access_token', 'bearer']):
            checks["jwt_implementation"] = True
        if any(x in code for x in ['jwt.encode', 'jwt.decode', 'create_access_token']):
            checks["jwt_implementation"] = True

        # JWTËøáÊúü
        if any(x in code for x in ['exp', 'expire', 'timedelta', 'ACCESS_TOKEN_EXPIRE']):
            checks["jwt_expiration"] = True

        # ËæìÂÖ•È™åËØÅ
        if any(x in code for x in ['Pydantic', 'BaseModel', 'EmailStr', 'validator', 'Field(']):
            checks["input_validation"] = True
        if 'class ' in code and 'BaseModel' in code:
            checks["input_validation"] = True

        # ORM‰ΩøÁî®
        if any(x in code for x in ['SQLAlchemy', 'declarative_base', 'Column', 'sessionmaker']):
            checks["orm_usage"] = True
        if '.query(' in code or 'db.query' in code:
            checks["orm_usage"] = True

        # Ê£ÄÊü•ÂéüÂßãSQL
        dangerous_patterns = [
            r'execute\s*\(\s*[\'\"]\s*SELECT',
            r'execute\s*\(\s*[\'\"]\s*INSERT',
            r'execute\s*\(\s*[\'\"]\s*UPDATE',
            r'execute\s*\(\s*[\'\"]\s*DELETE',
            r'f[\'\"]\s*SELECT.*{',
            r'%s.*%.*SELECT',
        ]
        for pattern in dangerous_patterns:
            if re.search(pattern, code, re.IGNORECASE):
                checks["no_raw_sql"] = False
                break

        # ÈîôËØØÂ§ÑÁêÜ
        if 'HTTPException' in code or 'raise ' in code:
            checks["error_handling"] = True
        if 'try:' in code and 'except' in code:
            checks["error_handling"] = True

        # ËÆ°ÁÆóÂàÜÊï∞
        scores = {
            "password_hashing": 8,
            "jwt_implementation": 8,
            "jwt_expiration": 4,
            "input_validation": 6,
            "orm_usage": 6,
            "no_raw_sql": 4,
            "error_handling": 4,
        }

        total = sum(scores[k] for k, v in checks.items() if v)

        return {
            "checks": checks,
            "score": total,
            "max_score": 40
        }

    def check_functionality(self, code: str) -> dict:
        """ÂäüËÉΩÂÆåÊï¥ÊÄßÊ£ÄÊü• (40ÂàÜ)"""
        checks = {
            "register_endpoint": False,     # 6ÂàÜ
            "login_endpoint": False,        # 6ÂàÜ
            "get_user_endpoint": False,     # 6ÂàÜ
            "update_user_endpoint": False,  # 6ÂàÜ
            "delete_user_endpoint": False,  # 6ÂàÜ
            "user_model": False,            # 5ÂàÜ
            "auth_dependency": False,       # 5ÂàÜ
        }

        # Á´ØÁÇπÊ£ÄÊü•
        endpoint_patterns = {
            "register_endpoint": [r'@app\.(post|route).*register', r'def.*register', r'/register'],
            "login_endpoint": [r'@app\.(post|route).*login', r'def.*login', r'/login'],
            "get_user_endpoint": [r'@app\.get.*user', r'def.*get.*user', r'GET.*user'],
            "update_user_endpoint": [r'@app\.put.*user', r'def.*update.*user', r'PUT.*user'],
            "delete_user_endpoint": [r'@app\.delete.*user', r'def.*delete.*user', r'DELETE.*user'],
        }

        code_lower = code.lower()
        for check_name, patterns in endpoint_patterns.items():
            for pattern in patterns:
                if re.search(pattern, code, re.IGNORECASE):
                    checks[check_name] = True
                    break

        # UserÊ®°Âûã
        if 'class User' in code or 'class UserModel' in code:
            checks["user_model"] = True
        if 'User(' in code and ('Column' in code or 'Field' in code):
            checks["user_model"] = True

        # ËÆ§ËØÅ‰æùËµñ
        if any(x in code for x in ['Depends(', 'get_current_user', 'oauth2_scheme', 'OAuth2']):
            checks["auth_dependency"] = True

        # ËÆ°ÁÆóÂàÜÊï∞
        scores = {
            "register_endpoint": 6,
            "login_endpoint": 6,
            "get_user_endpoint": 6,
            "update_user_endpoint": 6,
            "delete_user_endpoint": 6,
            "user_model": 5,
            "auth_dependency": 5,
        }

        total = sum(scores[k] for k, v in checks.items() if v)

        return {
            "checks": checks,
            "score": total,
            "max_score": 40
        }

    def check_quality(self, code: str) -> dict:
        """‰ª£Á†ÅË¥®ÈáèÊ£ÄÊü• (20ÂàÜ)"""
        checks = {
            "has_docstrings": False,        # 4ÂàÜ
            "has_type_hints": False,        # 4ÂàÜ
            "has_logging": False,           # 4ÂàÜ
            "proper_structure": False,      # 4ÂàÜ
            "response_models": False,       # 4ÂàÜ
        }

        # Docstrings
        if '"""' in code or "'''" in code:
            checks["has_docstrings"] = True

        # Type hints
        if ': str' in code or ': int' in code or '-> ' in code:
            checks["has_type_hints"] = True

        # Logging
        if 'logging' in code or 'logger' in code or 'log.' in code:
            checks["has_logging"] = True

        # ÁªìÊûÑ (Â§ö‰∏™ÂáΩÊï∞/Á±ª)
        tree = ast.parse(code)
        num_functions = len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)])
        num_classes = len([n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)])
        if num_functions >= 4 or (num_functions >= 2 and num_classes >= 2):
            checks["proper_structure"] = True

        # Response models
        if 'response_model' in code or 'ResponseModel' in code:
            checks["response_models"] = True
        if 'class ' in code and 'Response' in code:
            checks["response_models"] = True

        # ËÆ°ÁÆóÂàÜÊï∞
        scores = {
            "has_docstrings": 4,
            "has_type_hints": 4,
            "has_logging": 4,
            "proper_structure": 4,
            "response_models": 4,
        }

        total = sum(scores[k] for k, v in checks.items() if v)

        return {
            "checks": checks,
            "score": total,
            "max_score": 20
        }

    def evaluate_all(self, data_dir: Path) -> dict:
        """ËØÑ‰º∞ÊâÄÊúâÊ†∑Êú¨"""
        print("=" * 80)
        print("EXPERIMENT 2: API SERVICE EVALUATION")
        print("=" * 80)
        print(f"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        baseline_dir = data_dir / "baseline"
        multilayer_dir = data_dir / "multilayer"

        all_results = []

        # ËØÑ‰º∞Âü∫Á∫øÊ†∑Êú¨
        print("\n[1/3] Evaluating baseline samples...")
        for code_file in sorted(baseline_dir.glob("*.py")):
            sample_id = int(code_file.stem.split("_")[1])
            with open(code_file, 'r', encoding='utf-8') as f:
                code = f.read()

            result = self.evaluate_sample(code, sample_id, "baseline")
            all_results.append(result)
            print(f"  Sample {sample_id:03d}: Security={result['security_score']}/40, "
                  f"Func={result['functionality_score']}/40, "
                  f"Quality={result['quality_score']}/20, "
                  f"Total={result['total_score']}/100")

        # ËØÑ‰º∞Â§öÂ±ÇÊ¨°Ê†∑Êú¨
        print("\n[2/3] Evaluating multilayer samples...")
        for code_file in sorted(multilayer_dir.glob("*.py")):
            sample_id = int(code_file.stem.split("_")[1])
            with open(code_file, 'r', encoding='utf-8') as f:
                code = f.read()

            result = self.evaluate_sample(code, sample_id, "multilayer")
            all_results.append(result)
            print(f"  Sample {sample_id:03d}: Security={result['security_score']}/40, "
                  f"Func={result['functionality_score']}/40, "
                  f"Quality={result['quality_score']}/20, "
                  f"Total={result['total_score']}/100")

        # ‰øùÂ≠òÁªìÊûú
        print("\n[3/3] Saving results...")
        results_dir = data_dir / "evaluation_results"
        results_dir.mkdir(exist_ok=True)

        baseline_results = [r for r in all_results if r['prompt_type'] == 'baseline']
        multilayer_results = [r for r in all_results if r['prompt_type'] == 'multilayer']

        with open(results_dir / "baseline_results.json", 'w') as f:
            json.dump(baseline_results, f, indent=2)

        with open(results_dir / "multilayer_results.json", 'w') as f:
            json.dump(multilayer_results, f, indent=2)

        # ÁªüËÆ°
        stats = self.calculate_statistics(baseline_results, multilayer_results)
        with open(results_dir / "comparison_statistics.json", 'w') as f:
            json.dump(stats, f, indent=2)

        # ÊâìÂç∞ÁªìÊûú
        print("\n" + "=" * 80)
        print("EVALUATION RESULTS")
        print("=" * 80)

        print("\nüìä BASELINE GROUP:")
        print(f"  Syntax valid: {stats['baseline']['syntax_valid_count']}/{stats['baseline']['total']}")
        print(f"  Avg Security Score: {stats['baseline']['avg_security']:.2f}/40")
        print(f"  Avg Functionality Score: {stats['baseline']['avg_functionality']:.2f}/40")
        print(f"  Avg Quality Score: {stats['baseline']['avg_quality']:.2f}/20")
        print(f"  Avg Total Score: {stats['baseline']['avg_total']:.2f}/100")

        print("\nüìä MULTILAYER GROUP:")
        print(f"  Syntax valid: {stats['multilayer']['syntax_valid_count']}/{stats['multilayer']['total']}")
        print(f"  Avg Security Score: {stats['multilayer']['avg_security']:.2f}/40")
        print(f"  Avg Functionality Score: {stats['multilayer']['avg_functionality']:.2f}/40")
        print(f"  Avg Quality Score: {stats['multilayer']['avg_quality']:.2f}/20")
        print(f"  Avg Total Score: {stats['multilayer']['avg_total']:.2f}/100")

        print("\nüìà IMPROVEMENT:")
        print(f"  Security: +{stats['improvement']['security']:.2f} points")
        print(f"  Functionality: +{stats['improvement']['functionality']:.2f} points")
        print(f"  Quality: +{stats['improvement']['quality']:.2f} points")
        print(f"  Total: +{stats['improvement']['total']:.2f} points")

        print("\n" + "=" * 80)
        print(f"End time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"Results saved to: {results_dir}")
        print("=" * 80)

        return stats

    def calculate_statistics(self, baseline: list, multilayer: list) -> dict:
        """ËÆ°ÁÆóÁªüËÆ°Êï∞ÊçÆ"""
        def calc_group_stats(results):
            valid = [r for r in results if r['syntax_valid']]
            return {
                "total": len(results),
                "syntax_valid_count": len(valid),
                "syntax_valid_rate": len(valid) / len(results) if results else 0,
                "avg_security": sum(r['security_score'] for r in valid) / len(valid) if valid else 0,
                "avg_functionality": sum(r['functionality_score'] for r in valid) / len(valid) if valid else 0,
                "avg_quality": sum(r['quality_score'] for r in valid) / len(valid) if valid else 0,
                "avg_total": sum(r['total_score'] for r in valid) / len(valid) if valid else 0,
            }

        baseline_stats = calc_group_stats(baseline)
        multilayer_stats = calc_group_stats(multilayer)

        return {
            "baseline": baseline_stats,
            "multilayer": multilayer_stats,
            "improvement": {
                "security": multilayer_stats['avg_security'] - baseline_stats['avg_security'],
                "functionality": multilayer_stats['avg_functionality'] - baseline_stats['avg_functionality'],
                "quality": multilayer_stats['avg_quality'] - baseline_stats['avg_quality'],
                "total": multilayer_stats['avg_total'] - baseline_stats['avg_total'],
            }
        }


if __name__ == "__main__":
    evaluator = APISecurityEvaluator()
    data_dir = Path("/root/autodl-tmp/eoh/experiment2_api_service")
    evaluator.evaluate_all(data_dir)

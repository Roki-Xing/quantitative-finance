#!/usr/bin/env python3
"""
Experiment 1: Web Scraper Evaluator
è‡ªåŠ¨åŒ–è¯„ä¼°60ä¸ªç”Ÿæˆçš„Webçˆ¬è™«æ ·æœ¬

è¯„ä¼°ç»´åº¦:
1. è¯­æ³•æ­£ç¡®æ€§ (Syntax)
2. è¿è¡Œæ—¶ç¨³å®šæ€§ (Runtime)
3. åŠŸèƒ½å®Œæ•´æ€§ (Functionality)
4. å®‰å…¨æ€§ (Security)
"""

import os
import sys
import json
import subprocess
import traceback
from pathlib import Path
from datetime import datetime
import numpy as np
from typing import Dict, List, Tuple

# ============================================================================
# é…ç½®
# ============================================================================

EXPERIMENT_DIR = Path("/root/autodl-tmp/eoh/experiment1_web_scraper")
BASELINE_DIR = EXPERIMENT_DIR / "baseline"
MULTILAYER_DIR = EXPERIMENT_DIR / "multilayer"
RESULTS_DIR = EXPERIMENT_DIR / "evaluation_results"

# ============================================================================
# WebScraperEvaluatorç±»
# ============================================================================

class WebScraperEvaluator:
    """Webçˆ¬è™«ä»£ç è¯„ä¼°å™¨"""

    def __init__(self):
        self.test_url = "https://news.ycombinator.com"

    def test_syntax(self, code: str) -> Tuple[bool, str]:
        """
        è¯­æ³•æ£€æŸ¥

        Returns:
            (is_valid, error_message)
        """
        try:
            compile(code, "<string>", "exec")
            return True, None
        except SyntaxError as e:
            return False, f"SyntaxError at line {e.lineno}: {e.msg}"
        except Exception as e:
            return False, str(e)

    def test_runtime(self, code: str, timeout: int = 30) -> Tuple[bool, str]:
        """
        è¿è¡Œæ—¶æµ‹è¯• (ä¸å®é™…è¿è¡Œç½‘ç»œè¯·æ±‚ï¼Œåªæ£€æŸ¥æ˜¯å¦èƒ½æ‰§è¡Œ)

        Args:
            code: Pythonä»£ç 
            timeout: è¶…æ—¶æ—¶é—´(ç§’)

        Returns:
            (is_runnable, error_message)
        """
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_file = Path("/tmp/test_scraper.py")
        try:
            with open(temp_file, 'w', encoding='utf-8') as f:
                # æ³¨å…¥æµ‹è¯•æ¨¡å¼ï¼šæ›¿æ¢å®é™…ç½‘ç»œè¯·æ±‚ä¸ºmock
                test_code = self._inject_test_mode(code)
                f.write(test_code)

            # å°è¯•æ‰§è¡Œ
            result = subprocess.run(
                [sys.executable, str(temp_file)],
                capture_output=True,
                text=True,
                timeout=timeout,
            )

            if result.returncode == 0:
                return True, None
            else:
                return False, f"RuntimeError: {result.stderr[:500]}"

        except subprocess.TimeoutExpired:
            return False, "Timeout: Execution exceeded 30 seconds"
        except Exception as e:
            return False, f"Execution error: {str(e)}"
        finally:
            if temp_file.exists():
                temp_file.unlink()

    def _inject_test_mode(self, code: str) -> str:
        """
        æ³¨å…¥æµ‹è¯•æ¨¡å¼ï¼šé¿å…å®é™…ç½‘ç»œè¯·æ±‚

        å°†requests.get()æ›¿æ¢ä¸ºè¿”å›mock HTML
        """
        mock_html = '''
<html>
<tr class="athing" id="1">
    <span class="titleline"><a href="https://example.com">Test Article</a></span>
</tr>
<tr>
    <td class="subtext">
        <span class="score">100 points</span>
        <a>10 comments</a>
    </td>
</tr>
</html>
'''

        # åœ¨ä»£ç å¼€å¤´æ·»åŠ mock
        mock_code = f"""
# === TEST MODE: Mockç½‘ç»œè¯·æ±‚ ===
import requests as _real_requests

class MockResponse:
    def __init__(self):
        self.text = '''{mock_html}'''
        self.status_code = 200

    def raise_for_status(self):
        pass

class MockRequests:
    @staticmethod
    def get(*args, **kwargs):
        import time
        time.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        return MockResponse()

    exceptions = _real_requests.exceptions

# æ›¿æ¢requestsæ¨¡å—
import sys
sys.modules['requests'] = MockRequests()

# === åŸå§‹ä»£ç  ===
{code}
"""
        return mock_code

    def test_functionality(self, code: str) -> Tuple[float, Dict]:
        """
        åŠŸèƒ½æ€§æµ‹è¯•

        æ£€æŸ¥ä»£ç æ˜¯å¦åŒ…å«å¿…è¦çš„åŠŸèƒ½å…ƒç´ 

        Returns:
            (score, checks_dict)
        """
        checks = {
            "has_requests": "import requests" in code,
            "has_beautifulsoup": ("from bs4 import BeautifulSoup" in code or
                                 "import bs4" in code),
            "has_csv": "import csv" in code,
            "has_rate_limit": ("time.sleep" in code or "sleep(" in code),
            "has_error_handling": ("try:" in code and "except" in code),
            "has_user_agent": ("User-Agent" in code or "headers" in code),
            "saves_to_csv": (".csv" in code and "open(" in code),
            "has_functions": ("def " in code),
            "has_main": ("if __name__" in code or "def main" in code),
        }

        score = sum(checks.values()) / len(checks) * 100
        return score, checks

    def test_security(self, code: str) -> Tuple[float, List[str]]:
        """
        å®‰å…¨æ€§æµ‹è¯•

        æ£€æŸ¥æ½œåœ¨çš„å®‰å…¨éšæ‚£

        Returns:
            (score, issues_list)
        """
        issues = []

        # æ£€æŸ¥å±é™©æ“ä½œ
        if "os.system" in code or "subprocess.call" in code:
            issues.append("Dangerous system call detected")

        if "eval(" in code or "exec(" in code:
            issues.append("Dangerous eval/exec detected")

        # æ£€æŸ¥SQLæ³¨å…¥é£é™© (è™½ç„¶çˆ¬è™«ä¸å¸¸è§ï¼Œä½†æ£€æŸ¥ä¸€ä¸‹)
        if "execute(" in code and ("%" in code or ".format(" in code):
            issues.append("Potential SQL injection pattern")

        # æ£€æŸ¥rate limiting
        if "sleep" not in code and "time.sleep" not in code:
            issues.append("Missing rate limiting")

        # æ£€æŸ¥error handling
        if "try:" not in code:
            issues.append("Missing error handling")

        # æ£€æŸ¥User-Agent (ç¤¼è²Œçˆ¬è™«)
        if "User-Agent" not in code and "headers" not in code:
            issues.append("Missing User-Agent header")

        # è¯„åˆ†: æ¯ä¸ªé—®é¢˜-15åˆ†
        score = max(0, 100 - len(issues) * 15)
        return score, issues

    def comprehensive_eval(self, code: str, sample_id: int, group: str) -> Dict:
        """
        ç»¼åˆè¯„ä¼°

        Returns:
            å®Œæ•´çš„è¯„ä¼°ç»“æœå­—å…¸
        """
        print(f"  ğŸ” è¯„ä¼°æ ·æœ¬ #{sample_id} ({group})...")

        results = {
            "sample_id": sample_id,
            "group": group,
            "timestamp": datetime.now().isoformat(),
        }

        # 1. è¯­æ³•æ£€æŸ¥
        syntax_ok, syntax_error = self.test_syntax(code)
        results["syntax"] = {
            "pass": syntax_ok,
            "error": syntax_error
        }

        # 2. è¿è¡Œæ—¶æµ‹è¯• (ä»…åœ¨è¯­æ³•æ­£ç¡®æ—¶è¿è¡Œ)
        if syntax_ok:
            runtime_ok, runtime_error = self.test_runtime(code)
            results["runtime"] = {
                "pass": runtime_ok,
                "error": runtime_error
            }
        else:
            results["runtime"] = {
                "pass": False,
                "error": "Skipped due to syntax error"
            }

        # 3. åŠŸèƒ½æ€§æµ‹è¯•
        func_score, func_checks = self.test_functionality(code)
        results["functionality"] = {
            "score": func_score,
            "checks": func_checks
        }

        # 4. å®‰å…¨æ€§æµ‹è¯•
        sec_score, sec_issues = self.test_security(code)
        results["security"] = {
            "score": sec_score,
            "issues": sec_issues
        }

        # ç»¼åˆè¯„åˆ†
        if not syntax_ok:
            total_score = 0
        elif not results["runtime"]["pass"]:
            total_score = 25  # è¯­æ³•æ­£ç¡®å¾—25åˆ†
        else:
            total_score = (
                25 +  # è¯­æ³•æ­£ç¡®
                25 +  # è¿è¡Œæ—¶æ­£ç¡®
                func_score * 0.3 +  # åŠŸèƒ½æ€§30%
                sec_score * 0.2     # å®‰å…¨æ€§20%
            )

        results["total_score"] = round(total_score, 2)

        return results


# ============================================================================
# ä¸»è¯„ä¼°æµç¨‹
# ============================================================================

def evaluate_all_samples():
    """è¯„ä¼°æ‰€æœ‰æ ·æœ¬"""

    print("="*80)
    print("Experiment 1: Web Scraper Evaluation - Day 36")
    print("="*80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()

    # åˆ›å»ºç»“æœç›®å½•
    RESULTS_DIR.mkdir(parents=True, exist_ok=True)

    evaluator = WebScraperEvaluator()

    all_results = {
        "baseline": [],
        "multilayer": []
    }

    # ========================================================================
    # è¯„ä¼°åŸºçº¿ç»„ (30ä¸ªæ ·æœ¬)
    # ========================================================================

    print("ğŸ“Š è¯„ä¼°åŸºçº¿ç»„ (Baseline Prompt)...")
    baseline_files = sorted(BASELINE_DIR.glob("sample_*.py"))

    for i, code_file in enumerate(baseline_files, 1):
        try:
            with open(code_file, 'r', encoding='utf-8') as f:
                code = f.read()

            result = evaluator.comprehensive_eval(code, i, "baseline")
            result["code_file"] = str(code_file)
            all_results["baseline"].append(result)

            print(f"    âœ… #{i}: {result['total_score']:.1f}/100 "
                  f"(è¯­æ³•: {result['syntax']['pass']}, "
                  f"è¿è¡Œ: {result['runtime']['pass']}, "
                  f"åŠŸèƒ½: {result['functionality']['score']:.1f}, "
                  f"å®‰å…¨: {result['security']['score']:.1f})")

        except Exception as e:
            print(f"    âŒ #{i}: è¯„ä¼°å¤±è´¥ - {str(e)}")
            all_results["baseline"].append({
                "sample_id": i,
                "group": "baseline",
                "code_file": str(code_file),
                "error": str(e),
                "total_score": 0
            })

    # ========================================================================
    # è¯„ä¼°å¤šå±‚æ¬¡ç»„ (30ä¸ªæ ·æœ¬)
    # ========================================================================

    print("\nğŸ“Š è¯„ä¼°å¤šå±‚æ¬¡ç»„ (Multilayer Prompt)...")
    multilayer_files = sorted(MULTILAYER_DIR.glob("sample_*.py"))

    for i, code_file in enumerate(multilayer_files, 1):
        try:
            with open(code_file, 'r', encoding='utf-8') as f:
                code = f.read()

            result = evaluator.comprehensive_eval(code, i, "multilayer")
            result["code_file"] = str(code_file)
            all_results["multilayer"].append(result)

            print(f"    âœ… #{i}: {result['total_score']:.1f}/100 "
                  f"(è¯­æ³•: {result['syntax']['pass']}, "
                  f"è¿è¡Œ: {result['runtime']['pass']}, "
                  f"åŠŸèƒ½: {result['functionality']['score']:.1f}, "
                  f"å®‰å…¨: {result['security']['score']:.1f})")

        except Exception as e:
            print(f"    âŒ #{i}: è¯„ä¼°å¤±è´¥ - {str(e)}")
            all_results["multilayer"].append({
                "sample_id": i,
                "group": "multilayer",
                "code_file": str(code_file),
                "error": str(e),
                "total_score": 0
            })

    # ========================================================================
    # ä¿å­˜ç»“æœ
    # ========================================================================

    # ä¿å­˜åŸå§‹ç»“æœ
    baseline_results_file = RESULTS_DIR / "baseline_results.json"
    with open(baseline_results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results["baseline"], f, indent=2, ensure_ascii=False)

    multilayer_results_file = RESULTS_DIR / "multilayer_results.json"
    with open(multilayer_results_file, 'w', encoding='utf-8') as f:
        json.dump(all_results["multilayer"], f, indent=2, ensure_ascii=False)

    # ========================================================================
    # ç»Ÿè®¡åˆ†æ
    # ========================================================================

    print("\n" + "="*80)
    print("ğŸ“ˆ ç»Ÿè®¡åˆ†æ")
    print("="*80)

    stats = compute_statistics(all_results)

    # ä¿å­˜ç»Ÿè®¡ç»“æœ
    stats_file = RESULTS_DIR / "comparison_statistics.json"
    with open(stats_file, 'w', encoding='utf-8') as f:
        json.dump(stats, f, indent=2, ensure_ascii=False)

    # æ‰“å°å¯¹æ¯”è¡¨
    print_comparison_table(stats)

    # ========================================================================
    # å®Œæˆ
    # ========================================================================

    print("\n" + "="*80)
    print("âœ… è¯„ä¼°å®Œæˆ")
    print("="*80)
    print(f"åŸºçº¿ç»„ç»“æœ: {baseline_results_file}")
    print(f"å¤šå±‚æ¬¡ç»„ç»“æœ: {multilayer_results_file}")
    print(f"ç»Ÿè®¡åˆ†æ: {stats_file}")
    print(f"å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    print("ä¸‹ä¸€æ­¥: è¿è¡Œ experiment1_analyze_results.py è¿›è¡Œæ·±åº¦åˆ†æ")


def compute_statistics(all_results: Dict) -> Dict:
    """è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡"""

    def extract_scores(results_list):
        """ä»ç»“æœåˆ—è¡¨æå–å„é¡¹æŒ‡æ ‡"""
        scores = {
            "total_scores": [],
            "syntax_pass": [],
            "runtime_pass": [],
            "functionality_scores": [],
            "security_scores": []
        }

        for r in results_list:
            if "error" in r:
                continue

            scores["total_scores"].append(r["total_score"])
            scores["syntax_pass"].append(1 if r["syntax"]["pass"] else 0)
            scores["runtime_pass"].append(1 if r["runtime"]["pass"] else 0)
            scores["functionality_scores"].append(r["functionality"]["score"])
            scores["security_scores"].append(r["security"]["score"])

        return scores

    baseline_scores = extract_scores(all_results["baseline"])
    multilayer_scores = extract_scores(all_results["multilayer"])

    stats = {
        "baseline": {
            "sample_count": len(all_results["baseline"]),
            "avg_total_score": float(np.mean(baseline_scores["total_scores"])) if baseline_scores["total_scores"] else 0,
            "syntax_pass_rate": float(np.mean(baseline_scores["syntax_pass"]) * 100) if baseline_scores["syntax_pass"] else 0,
            "runtime_pass_rate": float(np.mean(baseline_scores["runtime_pass"]) * 100) if baseline_scores["runtime_pass"] else 0,
            "avg_functionality_score": float(np.mean(baseline_scores["functionality_scores"])) if baseline_scores["functionality_scores"] else 0,
            "avg_security_score": float(np.mean(baseline_scores["security_scores"])) if baseline_scores["security_scores"] else 0,
        },
        "multilayer": {
            "sample_count": len(all_results["multilayer"]),
            "avg_total_score": float(np.mean(multilayer_scores["total_scores"])) if multilayer_scores["total_scores"] else 0,
            "syntax_pass_rate": float(np.mean(multilayer_scores["syntax_pass"]) * 100) if multilayer_scores["syntax_pass"] else 0,
            "runtime_pass_rate": float(np.mean(multilayer_scores["runtime_pass"]) * 100) if multilayer_scores["runtime_pass"] else 0,
            "avg_functionality_score": float(np.mean(multilayer_scores["functionality_scores"])) if multilayer_scores["functionality_scores"] else 0,
            "avg_security_score": float(np.mean(multilayer_scores["security_scores"])) if multilayer_scores["security_scores"] else 0,
        }
    }

    # è®¡ç®—æ”¹è¿›
    stats["improvement"] = {
        "total_score": stats["multilayer"]["avg_total_score"] - stats["baseline"]["avg_total_score"],
        "syntax_pass_rate": stats["multilayer"]["syntax_pass_rate"] - stats["baseline"]["syntax_pass_rate"],
        "runtime_pass_rate": stats["multilayer"]["runtime_pass_rate"] - stats["baseline"]["runtime_pass_rate"],
        "functionality_score": stats["multilayer"]["avg_functionality_score"] - stats["baseline"]["avg_functionality_score"],
        "security_score": stats["multilayer"]["avg_security_score"] - stats["baseline"]["avg_security_score"],
    }

    return stats


def print_comparison_table(stats: Dict):
    """æ‰“å°å¯¹æ¯”è¡¨"""

    print("\n" + "="*80)
    print("å¯¹æ¯”ç»“æœ")
    print("="*80)
    print()

    print(f"{'æŒ‡æ ‡':<30} {'åŸºçº¿ç»„':<15} {'å¤šå±‚æ¬¡ç»„':<15} {'æ”¹è¿›':<15}")
    print("-" * 80)

    metrics = [
        ("æ ·æœ¬æ•°é‡", "sample_count", ""),
        ("å¹³å‡æ€»åˆ†", "avg_total_score", "{:.2f}"),
        ("è¯­æ³•é€šè¿‡ç‡ (%)", "syntax_pass_rate", "{:.1f}%"),
        ("è¿è¡Œé€šè¿‡ç‡ (%)", "runtime_pass_rate", "{:.1f}%"),
        ("åŠŸèƒ½è¯„åˆ†", "avg_functionality_score", "{:.2f}"),
        ("å®‰å…¨è¯„åˆ†", "avg_security_score", "{:.2f}"),
    ]

    for label, key, fmt in metrics:
        baseline_val = stats["baseline"][key]
        multilayer_val = stats["multilayer"][key]

        if fmt:
            baseline_str = fmt.format(baseline_val)
            multilayer_str = fmt.format(multilayer_val)
            if key != "sample_count":
                improvement = multilayer_val - baseline_val
                if "%" in fmt:
                    improvement_str = f"{improvement:+.1f}%"
                else:
                    improvement_str = f"{improvement:+.2f}"
            else:
                improvement_str = "-"
        else:
            baseline_str = str(baseline_val)
            multilayer_str = str(multilayer_val)
            improvement_str = "-"

        print(f"{label:<30} {baseline_str:<15} {multilayer_str:<15} {improvement_str:<15}")

    print()


if __name__ == "__main__":
    evaluate_all_samples()
